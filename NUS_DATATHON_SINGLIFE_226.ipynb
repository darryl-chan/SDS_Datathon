{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--6K014ymIrA"
      },
      "source": [
        "#  1 Preparing data\n",
        "\n",
        "installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#%pip install catboost\n",
        "#%pip install imbalanced_learn==0.11.0\n",
        "#%pip install imblearn==0.0\n",
        "#%pip install matplotlib==3.6.2\n",
        "#%pip install numpy==1.23.5\n",
        "#%pip install pandas==2.2.0\n",
        "#%pip install scikit_learn==1.4.0\n",
        "#%pip install seaborn==0.13.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlx0YVSsT2sM"
      },
      "source": [
        "## 1.1 Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGTc0RawVRat"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "from catboost import CatBoostClassifier\n",
        "from catboost import Pool\n",
        "filepath='./data/catB_train.parquet'\n",
        "df = pd.read_parquet(filepath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.2 Data Visualisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkZj0CVUT46j"
      },
      "source": [
        "## 1.2 Processing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We fill in the target columns that are missing with 0. 1 indicates that the client bought an insurance policy within 3 months and 0 means that the client did not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVwXZj_jTw40"
      },
      "outputs": [],
      "source": [
        "df[\"f_purchase_lh\"] = df[\"f_purchase_lh\"].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we plot a graph to show the proportion of clients who bought and did not buy within 3 months and we can see that the number is very skewed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.bar([0,1], df['f_purchase_lh'].value_counts(), tick_label=['Did not buy','Bought'])\n",
        "plt.title('Proportion of clients purchasing insurance or not within 3 months')\n",
        "plt.xlabel('Clients')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.replace('None',None,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['min_occ_date'].isna().value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We removed the client id as it is irrelevant for predicting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=['clntnum'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then created a column which indicates the client's age when they bought the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['adjusted_first_purchase'] = (datetime.datetime.now()-pd.to_datetime(df['min_occ_date']).fillna(pd.to_datetime(df['min_occ_date']).median())).dt.days/365\n",
        "df['adjusted_dob'] = (datetime.datetime.now()-pd.to_datetime(df['cltdob_fix']).fillna(pd.to_datetime(df['cltdob_fix']).median())).dt.days/365\n",
        "df=df.drop(columns=['cltdob_fix', 'min_occ_date'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['client_first_purchase_age']=df['adjusted_dob']-df['adjusted_first_purchase']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a histogram of the age for the clients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['adjusted_dob'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram of age')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This graph shows the histogram of client's age when they first purchased the policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(df['client_first_purchase_age'], bins=20, color='skyblue', edgecolor='black')\n",
        "plt.title('Histogram of client_first_purchase_age')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "negative_values = df[df['client_first_purchase_age'] < 0]\n",
        "negative_values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have a list of clients from different countries but we see that Singaporean and Malaysian clients are more likely to be in the client list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['ctrycode_desc'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We grouped the clients into Singaporeans, Malaysians and others and made them into categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['ctrycode_grouped'] = df['ctrycode_desc'].apply(lambda x: x if x in ['Singapore', 'Malaysia'] else 'Others')\n",
        "df=df.drop(columns=['ctrycode_desc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We see that Malaysians there are 85 Malaysians but none have bought an insurance within the last 3 months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df[df['ctrycode_grouped'] == 'Malaysia'].shape[0])\n",
        "print(sum(df[df['ctrycode_grouped'] == 'Malaysia']['f_purchase_lh']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Furthermore we see that there are 118 people who are not Singaporeans or Malaysians and none have bought an insurance within the last 3 months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df[df['ctrycode_grouped'].apply(lambda x:x not in ['Malaysia','Singapore'])].shape[0])\n",
        "print(sum(df[df['ctrycode_grouped'].apply(lambda x:x  not in ['Malaysia','Singapore'])]['f_purchase_lh']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we compare this with Singaporeans for which there are 17789 in the data and 710 of them bought an insurance within the last 3 months"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df[df['ctrycode_grouped'] == 'Singapore'].shape[0])\n",
        "print(sum(df[df['ctrycode_grouped'] == 'Singapore']['f_purchase_lh']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below is the proportion of clients by nationality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = ['Malaysians', 'Others', 'Singaporeans']\n",
        "values_set1 = [df[df['ctrycode_grouped'] == 'Malaysia'].shape[0], df[df['ctrycode_grouped'].apply(lambda x:x not in ['Malaysia','Singapore'])].shape[0], \n",
        "               df[df['ctrycode_grouped'] == 'Singapore'].shape[0]] \n",
        "\n",
        "data = pd.DataFrame({'Nationality': categories, 'Value': values_set1})\n",
        "\n",
        "colors = ['skyblue', 'salmon', 'lightgreen']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='Nationality', y='Value', data=data, palette=colors)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()-50),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=12)\n",
        "\n",
        "plt.title('Proportion of clients by nationality')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of everyone in the client list, only Singaporeans bought insurance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categories = ['Malaysians', 'Others', 'Singaporeans']\n",
        "values_set1 = [sum(df[df['ctrycode_grouped'] == 'Malaysia']['f_purchase_lh']), sum(df[df['ctrycode_grouped'].apply(lambda x:x not in ['Malaysia','Singapore'])]['f_purchase_lh']), \n",
        "               sum(df[df['ctrycode_grouped'] == 'Singapore']['f_purchase_lh'])]\n",
        "\n",
        "data = pd.DataFrame({'Nationality': categories, 'Value': values_set1})\n",
        "\n",
        "colors = ['skyblue', 'salmon', 'lightgreen']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "ax = sns.barplot(x='Nationality', y='Value', data=data, palette=colors)\n",
        "\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
        "                ha='center', va='center', xytext=(0, 10), textcoords='offset points', fontsize=12)\n",
        "plt.ylim(0, max(data['Value']) * 1.1)\n",
        "plt.title('Proportion of clients by nationality who bought insurance within the last 3 months')\n",
        "plt.ylabel('Value')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For race, we filled the missing values with others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['race_desc'] = df['race_desc'].fillna('Others')\n",
        "df['race_desc'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since there are only 2 values for sex we filled the missing values with the most common gender which happens to be male."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['cltsex_fix'] = df['cltsex_fix'].fillna(df['cltsex_fix'].mode()[0])\n",
        "df['cltsex_fix'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the flag columns which are binary variables which indicates the various risk and status indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flag_columns = [\n",
        "    'flg_substandard',\n",
        "    'flg_is_borderline_standard',\n",
        "    'flg_is_revised_term',\n",
        "    'flg_is_rental_flat',\n",
        "    'flg_has_health_claim',\n",
        "    'flg_has_life_claim',\n",
        "    'flg_gi_claim',\n",
        "    'flg_is_proposal',\n",
        "    'flg_with_preauthorisation',\n",
        "    'flg_is_returned_mail'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then sum up the risk indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[flag_columns] = df[flag_columns].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['flag_sum'] = df[flag_columns].sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=flag_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Similarly we sum up the consent to getting contacted and valid contact points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "consent_columns = [\n",
        "    'is_consent_to_mail',\n",
        "    'is_consent_to_email',\n",
        "    'is_consent_to_call',\n",
        "    'is_consent_to_sms'\n",
        "]\n",
        "\n",
        "validity_columns = [\n",
        "    'is_valid_dm',\n",
        "    'is_valid_email'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[consent_columns] = df[consent_columns].fillna(0)\n",
        "df[validity_columns] = df[validity_columns].fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['communication_consent_sum'] = df[consent_columns].sum(axis=1)\n",
        "df['communication_validity_sum'] = df[validity_columns].sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=[*consent_columns,*validity_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['hh_size_est'] = df['hh_size_est'].replace('>4', '5')\n",
        "df['hh_size_est'] = pd.to_numeric(df['hh_size_est'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then for the annual income of clients, we filled the missing values with 0 and Below 30K with 1 etc with the purpose of creating it into ordinal values. Which means that they have an order to it and 5 is the highest amount while 0 is the lowest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['annual_income_est'] = df['annual_income_est'].fillna(0)\n",
        "income_ranges = {\n",
        "    0:0,\n",
        "    'E.BELOW30K': 1,\n",
        "    'D.30K-60K': 2,\n",
        "    'C.60K-100K': 3,\n",
        "    'B.100K-200K': 4,\n",
        "    'A.ABOVE200K': 5\n",
        "}\n",
        "df['annual_income_est'] = df['annual_income_est'].apply(lambda x: income_ranges[x])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also have other categorical variables such as housewife retiree, is sg pr, is class 1 or 2 and dependent in at least another policy. We fill these missing values with the most common values found in the column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_fill_median = ['is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy']\n",
        "\n",
        "# Fill NaN values with the median in the specified columns (If there are more 1 than 0 then the value will become 1)\n",
        "df[columns_to_fill_median] = df[columns_to_fill_median].fillna(df[columns_to_fill_median].median())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then fill the "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['hh_size'] .fillna(df['hh_size'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we have hh_20 and pop_20, we keep both and removed hh_size and hh_size_est as the 2 latter can be calculated from the first two columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_fill_median = ['hh_20', 'pop_20']\n",
        "column_to_remove = ['hh_size_est', 'hh_size']\n",
        "df[columns_to_fill_median] = df[columns_to_fill_median].apply(pd.to_numeric, errors='coerce')\n",
        "df[columns_to_fill_median] = df[columns_to_fill_median].fillna(df[columns_to_fill_median].median(skipna=True))\n",
        "df=df.drop(columns=column_to_remove)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_columns = df.filter(like='ape_').columns.tolist() + \\\n",
        "                    df.filter(like='sumins_').columns.tolist() + \\\n",
        "                    df.filter(like='prempaid_').columns.tolist()\n",
        "\n",
        "df[filtered_columns] = df[filtered_columns].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We sum up the annual premium equivalent, sum insured and premium paid together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prefixes=['ape','sumins','prempaid']\n",
        "for prefix in prefixes:\n",
        "    columns_to_sum = df.filter(like=f'{prefix}').columns\n",
        "    df[f'sum_{prefix}'] = df[columns_to_sum].astype(float).sum(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=filtered_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "metric_columns = [\n",
        "    'n_months_last_bought_products',\n",
        "    'flg_latest_being_lapse',\n",
        "    'flg_latest_being_cancel',\n",
        "    'recency_lapse',\n",
        "    'recency_cancel',\n",
        "    'tot_inforce_pols',\n",
        "    'tot_cancel_pols',\n",
        "    'f_ever_declined_la'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in metric_columns:\n",
        "    print(f\"NaN counts for {col}: {df[col].isna().sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We remove number of months last bought (product code) as there were too many features to keep and n_months_last_bought_products is concise enough to tell us how many months ago did the client purchase an insurance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filtered_columns_last_bought = df.filter(like = 'n_months_last_bought').drop(columns = 'n_months_last_bought_products').columns.tolist()\n",
        "filtered_columns_last_bought"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop(columns=filtered_columns_last_bought)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We filled the missing values in affconnect with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "flags=['flg_affconnect_show_interest_ever', 'flg_affconnect_ready_to_buy_ever',\n",
        "       'flg_affconnect_lapse_ever']\n",
        "for f in flags:\n",
        "    print(df[f].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[flags]=df[flags].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We sum up the gi_claim_ever but since there were no claims for any clients we subsequently removed it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_to_sum = ['flg_gi_claim_29d435_ever', 'flg_gi_claim_058815_ever', 'flg_gi_claim_42e115_ever', 'flg_gi_claim_856320_ever']\n",
        "df['sum_gi_claim'] = df[columns_to_sum].sum(axis=1, skipna=True)\n",
        "df['sum_gi_claim'] = df['sum_gi_claim'].fillna(0)\n",
        "\n",
        "df=df.drop(columns=columns_to_sum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=['sum_gi_claim'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We turn affcon_visit_days into a boolean of whether a person visited."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['affcon_visited'] = df['affcon_visit_days'].fillna(0).apply(lambda x: int(x))\n",
        "df=df.drop(columns='affcon_visit_days')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cancel_cols=['f_ever_declined_la','tot_cancel_pols','recency_cancel','recency_lapse']\n",
        "df[cancel_cols]=df[cancel_cols].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We sum up ever bought."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "filter_f_ever_bought = df.filter(like='f_ever_bought').columns.to_list()\n",
        "df['sum_f_ever_bought']=df[filter_f_ever_bought].sum(axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "df=df.drop(columns=filter_f_ever_bought)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now sum up the success claims and the unsuccessful claims\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "claim_cnt_success = ['giclaim_cnt_success','hlthclaim_cnt_success']\n",
        "df['claim_cnt_success'] = df[claim_cnt_success].fillna(0).sum(axis=1, skipna=True)\n",
        "df=df.drop(columns=claim_cnt_success)\n",
        "claim_cnt_unsuccess = ['hlthclaim_cnt_unsuccess', 'giclaim_cnt_unsuccess']\n",
        "df['claim_cnt_unsuccess'] = df[claim_cnt_unsuccess].fillna(0).sum(axis=1, skipna=True)\n",
        "df=df.drop(columns=claim_cnt_unsuccess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the remaining columns, we drop it to minimize the features added to the model to reduce the overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columns_n_months_since_lapse = df.filter(like='n_months_since_lapse').columns.to_list()\n",
        "df=df.drop(columns=columns_n_months_since_lapse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "na_columns = df.columns[df.isna().any()]\n",
        "\n",
        "print(na_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df=df.drop(columns=na_columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering and data visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We performed a Principal component analysis and showed the clusterings in 3D to see if there are clear outliers or if the clusters are close together"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_features = scaler.fit_transform(df.drop(columns=df.select_dtypes(include=['string','object'])))\n",
        "num_clusters = 3\n",
        "\n",
        "\n",
        "kmeans = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "\n",
        "kmeans.fit(scaled_features)\n",
        "\n",
        "\n",
        "cluster_labels = kmeans.labels_\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "reduced_features = pca.fit_transform(scaled_features)\n",
        "\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"Explained Variance Ratio for PC{i+1}: {explained_variance_ratio[i]*100:.2f}%\")\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "scatter = ax.scatter(reduced_features[:, 0], reduced_features[:, 1], reduced_features[:, 2], c=cluster_labels, cmap='viridis')\n",
        "\n",
        "# Adding colorbar\n",
        "cbar = plt.colorbar(scatter)\n",
        "cbar.set_label('Cluster Labels')\n",
        "\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "\n",
        "plt.title('3D Scatter Plot of Clusters')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "colors = np.where(df[\"f_purchase_lh\"] == 1, 'red', 'green')\n",
        "\n",
        "alpha = np.where(df[\"f_purchase_lh\"] == 1, 1.0, 0.1)\n",
        "\n",
        "ax.scatter(reduced_features[:, 0], reduced_features[:, 1], reduced_features[:, 2], c=colors, alpha=alpha, s=1)\n",
        "\n",
        "ax.set_title('3D PCA for labels')\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X=df.drop(columns=[\"f_purchase_lh\"])\n",
        "y=df[\"f_purchase_lh\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bool_columns = X.columns[(X.eq(0) | X.eq(1)).all()]\n",
        "X[bool_columns] = X[bool_columns].astype(bool)\n",
        "categorical_features=X.select_dtypes(include=['object','string']).columns.tolist()\n",
        "categorical_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cat_feat=[*categorical_features,*bool_columns]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2SuJgLbm7p6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from sklearn.metrics import precision_recall_curve ,auc,roc_auc_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used Synthetic Minority Oversampling Technique (smote) to create synthethic data to form a more balanced dataset and in particular we used smoteNC instead of smote for categoical features as interpolating binary variables does not work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precision-Recall Threshold\n",
        "First, we have to find a lambda to act as the threshold.\n",
        "Given that the challenge is to dissect the dataset to uncover the critical touchpoints that contribute to customer drop-off, we want to reduce the number of false positives (i.e Number of customers who do not buy but we predict that they buy) to reduce customer churn while also balancing false negatives (i.e Number of customers who buy but we predict that they do not buy) to reduce inefficient marketing.\n",
        "Thus we use f1 score to devise a harmonic mean of precision and recall."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, y_train = SMOTENC(categorical_features=cat_feat).fit_resample(X_train, y_train)\n",
        "train_pool = Pool(X_train, label=y_train,cat_features=cat_feat)\n",
        "test_pool = Pool(X_val, label=y_val,cat_features=cat_feat)\n",
        "\n",
        "model = CatBoostClassifier(iterations=500, depth=8, learning_rate=0.1, loss_function='Logloss', verbose=False,cat_features=cat_feat)\n",
        "\n",
        "model.fit(train_pool, eval_set=test_pool)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_val, y_val_proba)\n",
        "\n",
        "auc_pr = auc(recall, precision)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall)\n",
        "optimal_threshold_index = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_threshold_index]\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'AUC-PR = {auc_pr:.2f}')\n",
        "plt.scatter(recall[optimal_threshold_index], precision[optimal_threshold_index], marker='o', color='r', label='Optimal Threshold')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "y_val_proba = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_val, y_val_proba)\n",
        "print(f'Optimal Threshold: {optimal_threshold}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used StratifiedKFold for imbalanced datasets because it maintains the original class distribution in each cross-validation fold. This ensures a more accurate representation during model evaluation, enhancing the reliability of the results. It helps the model generalize better across different class imbalances in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Number of folds\n",
        "n_splits = 5\n",
        "# Initialise Stratified KFold\n",
        "stratified_kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "models_dict = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for fold,(train_index, test_index) in enumerate(stratified_kf.split(X, y)):\n",
        "    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n",
        "    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n",
        "    X_train, y_train = SMOTENC(categorical_features=cat_feat).fit_resample(X_train, y_train)\n",
        "    train_pool = Pool(X_train, label=y_train,cat_features=cat_feat)\n",
        "    test_pool = Pool(X_val, label=y_val,cat_features=cat_feat)\n",
        "\n",
        "    model = CatBoostClassifier(iterations=500, depth=8, learning_rate=0.1, loss_function='Logloss', verbose=False,cat_features=cat_feat)\n",
        "    model.fit(train_pool, eval_set=test_pool)\n",
        "    models_dict[f\"Fold_{fold+1}\"] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict(models, X_input, cat_feat,thres=0.5):\n",
        "    input_pool = Pool(X_input, cat_features=cat_feat)\n",
        "    predictions = [ model.predict(input_pool) for model in models.values()]\n",
        "    average_prediction = sum(predictions) / len(predictions)\n",
        "\n",
        "    return average_prediction >= thres"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def testing_hidden_data(hidden_data: pd.DataFrame) -> list:\n",
        "    df=hidden_data\n",
        "    df.replace('None',None,inplace=True)\n",
        "    df['adjusted_first_purchase'] = (datetime.datetime.now()-pd.to_datetime(df['min_occ_date']).fillna(pd.to_datetime(df['min_occ_date']).median())).dt.days/365\n",
        "    df['adjusted_dob'] = (datetime.datetime.now()-pd.to_datetime(df['cltdob_fix']).fillna(pd.to_datetime(df['cltdob_fix']).median())).dt.days/365\n",
        "    df['client_first_purchase_age']=df['adjusted_dob']-df['adjusted_first_purchase']\n",
        "    df['ctrycode_desc']=df['ctrycode_desc'].fillna('Singapore')\n",
        "    df['race_desc'] = df['race_desc'].fillna('Others')\n",
        "    \n",
        "    keys_to_drop = ['clntnum', 'min_occ_date', 'cltdob_fix']\n",
        "    df = df.drop(columns=keys_to_drop)\n",
        "    \n",
        "    flag_columns = [\n",
        "        'flg_substandard',\n",
        "        'flg_is_borderline_standard',\n",
        "        'flg_is_revised_term',\n",
        "        'flg_is_rental_flat',\n",
        "        'flg_has_health_claim',\n",
        "        'flg_has_life_claim',\n",
        "        'flg_gi_claim',\n",
        "        'flg_is_proposal',\n",
        "        'flg_with_preauthorisation',\n",
        "        'flg_is_returned_mail'\n",
        "    ]\n",
        "    \n",
        "    df[flag_columns] = df[flag_columns].fillna(0)\n",
        "    df['flag_sum'] = df[flag_columns].sum(axis=1)\n",
        "    df=df.drop(columns=flag_columns)\n",
        "    df['cltsex_fix'] = df['cltsex_fix'].fillna(df['cltsex_fix'].mode()[0])\n",
        "    consent_columns = [\n",
        "    'is_consent_to_mail',\n",
        "    'is_consent_to_email',\n",
        "    'is_consent_to_call',\n",
        "    'is_consent_to_sms'\n",
        "    ]\n",
        "\n",
        "    validity_columns = [\n",
        "        'is_valid_dm',\n",
        "        'is_valid_email'\n",
        "    ]\n",
        "    \n",
        "    df[consent_columns] = df[consent_columns].fillna(0)\n",
        "    df[validity_columns] = df[validity_columns].fillna(0)\n",
        "    df['communication_consent_sum'] = df[consent_columns].sum(axis=1)\n",
        "    df['communication_validity_sum'] = df[validity_columns].sum(axis=1)\n",
        "    df=df.drop(columns=[*consent_columns,*validity_columns])\n",
        "    claim_cnt_success = ['giclaim_cnt_success','hlthclaim_cnt_success']\n",
        "    df['claim_cnt_success'] = df[claim_cnt_success].fillna(0).sum(axis=1, skipna=True)\n",
        "    df=df.drop(columns=claim_cnt_success)\n",
        "    claim_cnt_unsuccess = ['hlthclaim_cnt_unsuccess', 'giclaim_cnt_unsuccess']\n",
        "    df['claim_cnt_unsuccess'] = df[claim_cnt_unsuccess].fillna(0).sum(axis=1, skipna=True)\n",
        "    df=df.drop(columns=claim_cnt_unsuccess)\n",
        "    col_to_drop = ['flg_gi_claim_29d435_ever', 'flg_gi_claim_058815_ever', 'flg_gi_claim_42e115_ever', 'flg_gi_claim_856320_ever']\n",
        "    df=df.drop(columns=col_to_drop)\n",
        "    \n",
        "    \n",
        "    columns_n_months_since_lapse = df.filter(like='n_months_since_lapse').columns.to_list()\n",
        "    df=df.drop(columns=columns_n_months_since_lapse)\n",
        "    \n",
        "    size_drop = ['hh_size_est', 'hh_size']\n",
        "    \n",
        "    df=df.drop(columns=size_drop)\n",
        "\n",
        "    df['annual_income_est'] = df['annual_income_est'].fillna(0)\n",
        "    \n",
        "    income_ranges = {\n",
        "        0:0,\n",
        "        'E.BELOW30K': 1,\n",
        "        'D.30K-60K': 2,\n",
        "        'C.60K-100K': 3,\n",
        "        'B.100K-200K': 4,\n",
        "        'A.ABOVE200K': 5\n",
        "    }\n",
        "    \n",
        "    df['annual_income_est'] = df['annual_income_est'].apply(lambda x: income_ranges[x])\n",
        "    columns_to_fill_median = ['is_housewife_retiree', 'is_sg_pr', 'is_class_1_2', 'is_dependent_in_at_least_1_policy','hh_20', 'pop_20']\n",
        "    df[columns_to_fill_median] = df[columns_to_fill_median].apply(pd.to_numeric, errors='coerce')\n",
        "    df[columns_to_fill_median] = df[columns_to_fill_median].fillna(df[columns_to_fill_median].median())\n",
        "    df[columns_to_fill_median] = df[columns_to_fill_median].fillna(df[columns_to_fill_median].median(skipna=True))\n",
        "    filtered_columns = df.filter(like='ape_').columns.tolist() + \\\n",
        "                    df.filter(like='sumins_').columns.tolist() + \\\n",
        "                    df.filter(like='prempaid_').columns.tolist()\n",
        "    df[filtered_columns] = df[filtered_columns].fillna(0)\n",
        "    prefixes=['ape','sumins','prempaid']\n",
        "    for prefix in prefixes:\n",
        "        columns_to_sum = df.filter(like=f'{prefix}').columns\n",
        "        df[f'sum_{prefix}'] = df[columns_to_sum].astype(float).sum(axis=1)\n",
        "    df=df.drop(columns=filtered_columns)\n",
        "    flags=['flg_affconnect_show_interest_ever', 'flg_affconnect_ready_to_buy_ever',\n",
        "       'flg_affconnect_lapse_ever']\n",
        "    df[flags]=df[flags].fillna(0)\n",
        "\n",
        "    df['affcon_visited'] = df['affcon_visit_days'].fillna(0).apply(lambda x: int(x))\n",
        "    df=df.drop(columns='affcon_visit_days')\n",
        "    cancel_cols=['f_ever_declined_la','tot_cancel_pols','recency_cancel','recency_lapse']\n",
        "    df[cancel_cols]=df[cancel_cols].fillna(0)\n",
        "    df['ctrycode_grouped'] = df['ctrycode_desc'].apply(lambda x: x if x in ['Singapore', 'Malaysia'] else 'Others')\n",
        "    na_columns = df.columns[df.isna().any()]\n",
        "    df=df.drop(columns=na_columns)\n",
        "    \n",
        "    filter_f_ever_bought = df.filter(like='f_ever_bought').columns.to_list()\n",
        "    df['sum_f_ever_bought']=df[filter_f_ever_bought].sum(axis=1)\n",
        "    df=df.drop(columns=filter_f_ever_bought)\n",
        "    \n",
        "    bool_columns = df.columns[(df.eq(0) | df.eq(1)).all()]\n",
        "    df[bool_columns] = df[bool_columns].astype(bool)\n",
        "    categorical_features=df.select_dtypes(include=['object','string']).columns.tolist()\n",
        "    cat_feat=[*categorical_features,*bool_columns]\n",
        "    result = predict(models_dict,df,cat_feat,optimal_threshold)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_parquet(filepath)\n",
        "try:\n",
        "    test_df = test_df.drop(columns=[\"f_purchase_lh\"])\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "print(testing_hidden_data(test_df))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
